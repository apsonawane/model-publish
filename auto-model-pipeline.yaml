trigger:
 - main 

schedules: 

 - cron: "0 0 * * *" 

   displayName: Daily midnight build 

   branches: 

     include: 
       - main 

   always: true 

parameters:
 - name: model_name
   displayName: mode name on huggingface website
   type: string
   default: meta-llama/Llama-3.2-3B-Instruct

pool: onnxruntime-Linux-GPU-T4
steps:
 - task: UsePythonVersion@0
   inputs:
     versionSpec: '3.10'
     architecture: 'x64'

 - bash: |
     pip install -r llama-requirements.txt

   displayName: install the necessary packages
   workingDirectory: $(Build.SourcesDirectory)

 - bash: |
     echo Authenticate with Huggingface repository 

     huggingface-cli login --token $(hf_token)

   displayName: "Authentication to Huggingface repo" 

 - bash: |
    pip list 
   displayName: "dump pip list"

 #- bash: |
 #   nvidia-smi
 #  displayName: "dump nvidia-smi"

 - script: | 

     python $(Build.SourcesDirectory)/llama_model_builder.py --model_name '${{ parameters.model_name }}' --output_dir '$(Build.BinariesDirectory)'

   displayName: "Convert Huggingface model to ONNX model"
     
   workingDirectory: $(Build.SourcesDirectory)

 - task: AzureCLI@2
   displayName: 'upload model to AzureML registry'
   inputs:
     azureSubscription: AIInfraBuildOnnxRuntimeOSS
     scriptLocation: inlineScript
     scriptType: bash
     inlineScript: |
       curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash 
       az extension add -n ml -y
       az ml data create --name llama32-3b-instruct-for-test-only --version 1 --path ./model --registry-name Genai-model-registry --description "llama3.2-3b-instruct-test-only from pipeline"
   workingDirectory: $(Build.BinariesDirectory)/models/${{ parameters.model_name }}/output_model 
   